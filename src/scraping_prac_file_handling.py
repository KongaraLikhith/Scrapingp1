# -*- coding: utf-8 -*-
"""scraping prac_file_handling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lpNir7HI3smRH62YJoCOCsOk-Z1ox6Ea
"""
## starting code is how basic file handling is done.
f = open("firstfile.txt","w")
f.write("hello this is my first file\n")
f.write("i am writing matter into itn")
f.write("hi there 3 rd line")
f.close()

f1=open("firstfile.txt","r")
content = f1.read()
f1.close()
print(content)

f1=open("firstfile.txt","r")
content = f1.read()
print(content)
f1.close()
f1.write("experimenting with the close and mode")

f2 = open("firstfile.txt","r")  # so when we use range as "for i in f2.read() - we get one by one character on new line when we print"
for i in f2:                    # if we use the file f2 object it gives is nrml lines.
  print(i)
f2.close()

# ok now insteading of using close after everytime we open we use "with"

with open("firstfile.txt","r") as flobj:
  print(flobj.read())

# this with block acts as a way like everything we need is to be witten in the with block

with open("firstfile.txt","r") as f3:
  print(f3)
# print(f3)

with open("firstfile.txt","r") as f3:  # this works but file it is pointing to is closed.
  print(f3)
print(f3)

with open("firstfile.txt","r") as f3:
  for i in f3:
    print(i)
print(f3)

with open("firstfile.txt","w") as fwrt:
  fwrt.write("write mode overwrites the file")
  print(fwrt.read())

# Write first
with open("firstfile.txt", "w") as fwrt:
    fwrt.write("write mode overwrites the file")

# Read after
with open("firstfile.txt", "r") as frd:
    print(frd.read())

# but where as with append we can add additional information to the file.

with open("secondfile.txt","a") as fa:
  fa.write("this is the 2nd file and iam appending")
with open("secondfile.txt","r") as fa:
  print(fa.read())


# -----------------------------------------------------------------------------------------------------------
# from here on we write code for scraping data from a site.
import requests

url = "http://quotes.toscrape.com/"
response = requests.get(url)  # Send GET request to the webpage
print("Status code:", response.status_code)  # 200 means success

html = response.text  # The HTML content of the page
print(html[:500])  # print first 500 characters

from bs4 import BeautifulSoup

soup = BeautifulSoup(html, "html.parser")  # parse HTML
print(soup.prettify()[:500])  # print formatted HTML

quotes = soup.find_all("span", class_="text")
authors = soup.find_all("small", class_="author")

for i in range(len(quotes)):
    print(quotes[i].text, "-", authors[i].text)

import pandas as pd

data = [{"Quote": quotes[i].text, "Author": authors[i].text} for i in range(len(quotes))]

df = pd.DataFrame(data)
df.to_csv("quotes.csv", index=False)
print("✅ Quotes saved to quotes.csv")

all_data = []

for page in range(1, 4):  # scrape first 3 pages
    url = f"http://quotes.toscrape.com/page/{page}/"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")

    quotes = soup.find_all("span", class_="text")
    authors = soup.find_all("small", class_="author")

    page_data = [{"Quote": quotes[i].text, "Author": authors[i].text} for i in range(len(quotes))]
    all_data.extend(page_data)

df = pd.DataFrame(all_data)
df.to_csv("quotes_pages1-3.csv", index=False)
print("✅ All pages saved")

# !pip freeze > requirements.txt---------------  # this is just an optional one while moving it too git we need requirements.txt 
# for the libraries used.